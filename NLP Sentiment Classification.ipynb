{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b66d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225433e",
   "metadata": {},
   "source": [
    "## Part 0 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cdc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efb85a",
   "metadata": {},
   "source": [
    "## Part 1 Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a6291a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# List all available pre-trained models\n",
    "available_models = api.info()['models'].keys()\n",
    "print(available_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b196a4",
   "metadata": {},
   "source": [
    "### Loading 'word2vec-google-news-300' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5f4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model (300-dimensional vectors)\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f94e493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8530\n",
      "Element 0: text\n",
      "Element 1: label\n",
      "'text' : the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "'label' : 1\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "# View the first sample in the dataset\n",
    "first_sample = train_dataset[0]\n",
    "for i, key in enumerate(first_sample):\n",
    "    print(f\"Element {i}: {key}\")\n",
    "\n",
    "for key, value in first_sample.items():\n",
    "    print(f\"'{key}' : {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e981acb",
   "metadata": {},
   "source": [
    "### (a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7feed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18951\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    vocab_counter.update(sentence.split())  # Split sentences into words\n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67723ef9-09dd-4672-839e-d79184356444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18223\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = text.split()  \n",
    "    return tokens\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    tokens = preprocess_text(sentence)\n",
    "    vocab_counter.update(tokens)  \n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38974c",
   "metadata": {},
   "source": [
    "### (b) How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c65b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 3334\n",
      "Sample of 10 OOV words: ['to', '21st', 'centurys', 'and', 'a', 'jeanclaud', 'damme', 'segal', 'of', 'cowriterdirector']\n"
     ]
    }
   ],
   "source": [
    "oov_words = []\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word2vec_model:\n",
    "        oov_words.append(word)  # Use pre-trained Word2Vec vector\n",
    "        \n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_words)))\n",
    "print(\"Sample of 10 OOV words: \" + str(oov_words[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645726e4",
   "metadata": {},
   "source": [
    "### (c) Strategy to mitigate limitation of OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefdf64",
   "metadata": {},
   "source": [
    "Each OOV word can be represented as a bag of character N-grams. Embeddings are then generated based on these N-grams. By representing words as combinations of n-grams, FastText can generalize better across similar words. For example, if the model has seen \"apple,\" it can infer meaningful representations for related words like \"apples,\" \"applet,\" and even misspellings or variations.FastTextâ€™s n-gram approach can capture semantic similarities between words that share similar character patterns. For instance, \"cat\" and \"cats\" will share common n-grams, leading to embeddings that are close to each other in the vector space. For instance:\n",
    "The word \"unhappiness\" can be broken down into n-grams like \"un,\" \"happi,\" \"ness,\" etc.\n",
    "This enables FastText to understand that \"unhappy\" and \"happiness\" share a common root, even if those specific words were not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a447b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Load the pre-trained FastText model (English) from Gensim's API\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f13b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 2679\n",
      "Some OOV words: ['<PAD>', '<UNK>', 'jeanclaud', 'damme', 'cowriterdirector', 'tolkiens', 'middleearth', 'tootepid', 'wisegirls', 'familyoriented']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load your models and define vocab\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>'] + vocab  # Adding a padding token and a unk token\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))  # Initialize embedding matrix\n",
    "\n",
    "# Create a clean_word-to-index dictionary for your vocabulary\n",
    "word_to_idx = { word : idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "oov_random_embeds = []\n",
    "\n",
    "# Fill the embedding matrix\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[idx] = word2vec_model[word]  # Use Word2Vec vector\n",
    "    elif word in fasttext_model:\n",
    "        embedding_matrix[idx] = fasttext_model[word]  # Use FastText vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random vector for OOV words\n",
    "        oov_random_embeds.append(word)\n",
    "\n",
    "        \n",
    "# Check for OOV words\n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_random_embeds)))\n",
    "print(\"Some OOV words:\", oov_random_embeds[:10])  # Print the first 10 OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d2253-9ece-4fc3-87cb-68892e5e605e",
   "metadata": {},
   "source": [
    "## Part 2 Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200fac77-0b01-4fdc-9803-d8add50021ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word2vec vectors : (300,)\n"
     ]
    }
   ],
   "source": [
    "#print(\"Shape of word2vec vectors : \" + str(word2vec_model['great'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "095a88ff-20f4-418d-a40f-3c155ceba1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 59\n",
      "Min sequence length: 1\n",
      "Average sequence length: 20.99284876905041\n",
      "[12, 13, 14, 15, 16, 17, 12, 18, 19, 20, 21, 22, 23, 24, 25, 16, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    }
   ],
   "source": [
    "def find_seq_length(sentence):\n",
    "    sentencelist = sentence.split()\n",
    "    return len(sentencelist)\n",
    "\n",
    "def encode(sentence):\n",
    "    encoded_list = []\n",
    "    tokens = preprocess_text(sentence)\n",
    "    for token in tokens: # truncate? [:max_sequence_length]\n",
    "        if token in word_to_idx:\n",
    "            encoded_list.append(word_to_idx[token])\n",
    "        else:\n",
    "            encoded_list.append(1)# index of <UNK>\n",
    "\n",
    "    # padding : .append(0)\n",
    "    return encoded_list\n",
    "\n",
    "maxlength = 0\n",
    "minlength = float('inf')\n",
    "total_length = 0\n",
    "for i in range(len(train_dataset)):\n",
    "    cur = find_seq_length(train_dataset[i]['text'])\n",
    "    total_length += cur\n",
    "    if cur > maxlength:\n",
    "        maxlength = cur\n",
    "    if cur < minlength:\n",
    "        minlength = cur\n",
    "\n",
    "print(\"Max sequence length: \" + str(maxlength))\n",
    "print(\"Min sequence length: \" + str(minlength))\n",
    "print(\"Average sequence length: \" + str(total_length/len(train_dataset)))\n",
    "\n",
    "print(encode(train_dataset[0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59fe6d20-699d-465e-b0f0-447aff256505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word2vec vectors : (300,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Shape of word2vec vectors : \" + str(word2vec_model['great'].shape))\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32     \n",
    "#time_size = max length ?\n",
    "feature_size =  word2vec_model['great'].shape[0]\n",
    "num_classes = 2    \n",
    "learning_rate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1041fba0-5829-4d6c-b1c7-5b7ead06e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = feature_size\n",
    "hidden_size = feature_size * 4  # Size of the hidden layer\n",
    "output_size = num_classes  # Number of output classes\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weight matrices\n",
    "        self.Wx = nn.Parameter(torch.Tensor(input_size, hidden_size))  # Input to hidden\n",
    "        self.Wh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))  # Hidden to hidden\n",
    "        self.b = nn.Parameter(torch.Tensor(hidden_size))  # Bias\n",
    "\n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.kaiming_uniform_(param)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # x: input at the current time step (batch_size, input_size)\n",
    "        # h: hidden state from the previous time step (batch_size, hidden_size)\n",
    "        h_next = torch.tanh(torch.mm(x, self.Wx) + torch.mm(h, self.Wh) + self.b)\n",
    "        return h_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c8f5d3c-5b5c-46c6-ab95-6905801c0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = SimpleRNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        max_length = x.size(1)\n",
    "\n",
    "        # Initialize hidden state (h_0)\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Iterate over each time step, but only for valid lengths\n",
    "        for t in range(max_length):\n",
    "            # Only process valid time steps based on lengths\n",
    "            for i in range(batch_size):\n",
    "                if t < lengths[i]:\n",
    "                    h[i] = self.rnn_cell(x[i, t, :], h[i])  # Update the hidden state for valid sequences\n",
    "\n",
    "        # Use the hidden states from the last valid time step for classification\n",
    "        # Take the last hidden state for each sequence based on lengths\n",
    "        final_h = h\n",
    "\n",
    "        # Use the final hidden state for the output layer\n",
    "        out = self.fc(final_h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "146e82c6-f2fd-46df-a316-f0c8ab7e71db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fan in and fan out can not be computed for tensor with fewer than 2 dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m output_size \u001b[38;5;241m=\u001b[39m num_classes  \u001b[38;5;66;03m# Number of output classes\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleRNN(input_size, hidden_size, output_size)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Loss and optimizer\u001b[39;00m\n\u001b[0;32m     27\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[1;32mIn[41], line 5\u001b[0m, in \u001b[0;36mSimpleRNN.__init__\u001b[1;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(SimpleRNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m hidden_size\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_cell \u001b[38;5;241m=\u001b[39m SimpleRNNCell(input_size, hidden_size)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size, output_size)\n",
      "Cell \u001b[1;32mIn[38], line 22\u001b[0m, in \u001b[0;36mSimpleRNNCell.__init__\u001b[1;34m(self, input_size, hidden_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mTensor(hidden_size))  \u001b[38;5;66;03m# Bias\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n",
      "Cell \u001b[1;32mIn[38], line 26\u001b[0m, in \u001b[0;36mSimpleRNNCell.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m---> 26\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_uniform_(param)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\init.py:454\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[0;32m    452\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing zero-element tensors is a no-op\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m--> 454\u001b[0m fan \u001b[38;5;241m=\u001b[39m _calculate_correct_fan(tensor, mode)\n\u001b[0;32m    455\u001b[0m gain \u001b[38;5;241m=\u001b[39m calculate_gain(nonlinearity, a)\n\u001b[0;32m    456\u001b[0m std \u001b[38;5;241m=\u001b[39m gain \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(fan)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\init.py:402\u001b[0m, in \u001b[0;36m_calculate_correct_fan\u001b[1;34m(tensor, mode)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_modes:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported, please use one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 402\u001b[0m fan_in, fan_out \u001b[38;5;241m=\u001b[39m _calculate_fan_in_and_fan_out(tensor)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fan_in \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfan_in\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m fan_out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\init.py:318\u001b[0m, in \u001b[0;36m_calculate_fan_in_and_fan_out\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    316\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdim()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dimensions \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFan in and fan out can not be computed for tensor with fewer than 2 dimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    320\u001b[0m num_input_fmaps \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    321\u001b[0m num_output_fmaps \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Fan in and fan out can not be computed for tensor with fewer than 2 dimensions"
     ]
    }
   ],
   "source": [
    "## Example Usage  NOT HOW WE ACTUALLY USE IT\n",
    "\n",
    "# Sample data\n",
    "batch_size = 4\n",
    "max_sequence_length = 5\n",
    "feature_size = 300\n",
    "num_classes = 2\n",
    "\n",
    "# Randomly generated input (batch_size, max_sequence_length, feature_size)\n",
    "input_data = torch.randn(batch_size, max_sequence_length, feature_size)\n",
    "\n",
    "# Lengths of the actual sequences\n",
    "lengths = torch.tensor([5, 3, 4, 2])  # Different lengths for each sentence\n",
    "\n",
    "# Random labels (0 or 1 for binary classification)\n",
    "labels = torch.tensor([0, 1, 0, 1])\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = feature_size\n",
    "hidden_size = 128  # Size of the hidden layer\n",
    "output_size = num_classes  # Number of output classes\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (for demonstration, just one iteration)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_data, lengths)\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# Backward pass and optimization\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_data, lengths)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(\"Predicted labels:\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47acabc-a0c3-4e1f-b315-be23e71103af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
