{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b66d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225433e",
   "metadata": {},
   "source": [
    "## Part 0 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cdc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efb85a",
   "metadata": {},
   "source": [
    "## Part 1 Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a6291a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# List all available pre-trained models\n",
    "available_models = api.info()['models'].keys()\n",
    "print(available_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b196a4",
   "metadata": {},
   "source": [
    "### Loading 'word2vec-google-news-300' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5f4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model (300-dimensional vectors)\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f94e493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8530\n",
      "Element 0: text\n",
      "Element 1: label\n",
      "'text' : the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "'label' : 1\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "# View the first sample in the dataset\n",
    "first_sample = train_dataset[0]\n",
    "for i, key in enumerate(first_sample):\n",
    "    print(f\"Element {i}: {key}\")\n",
    "\n",
    "for key, value in first_sample.items():\n",
    "    print(f\"'{key}' : {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e981acb",
   "metadata": {},
   "source": [
    "### (a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df7feed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18951\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    vocab_counter.update(sentence.split())  # Split sentences into words\n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67723ef9-09dd-4672-839e-d79184356444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18223\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = text.split()  \n",
    "    return tokens\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    tokens = preprocess_text(sentence)\n",
    "    vocab_counter.update(tokens)  \n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38974c",
   "metadata": {},
   "source": [
    "### (b) How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c3c65b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 3334\n",
      "Sample of 10 OOV words: ['to', '21st', 'centurys', 'and', 'a', 'jeanclaud', 'damme', 'segal', 'of', 'cowriterdirector']\n"
     ]
    }
   ],
   "source": [
    "oov_words = []\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word2vec_model:\n",
    "        oov_words.append(word)  # Use pre-trained Word2Vec vector\n",
    "        \n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_words)))\n",
    "print(\"Sample of 10 OOV words: \" + str(oov_words[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645726e4",
   "metadata": {},
   "source": [
    "### (c) Strategy to mitigate limitation of OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefdf64",
   "metadata": {},
   "source": [
    "Each OOV word can be represented as a bag of character N-grams. Embeddings are then generated based on these N-grams. By representing words as combinations of n-grams, FastText can generalize better across similar words. For example, if the model has seen \"apple,\" it can infer meaningful representations for related words like \"apples,\" \"applet,\" and even misspellings or variations.FastTextâ€™s n-gram approach can capture semantic similarities between words that share similar character patterns. For instance, \"cat\" and \"cats\" will share common n-grams, leading to embeddings that are close to each other in the vector space. For instance:\n",
    "The word \"unhappiness\" can be broken down into n-grams like \"un,\" \"happi,\" \"ness,\" etc.\n",
    "This enables FastText to understand that \"unhappy\" and \"happiness\" share a common root, even if those specific words were not seen during training.\n",
    "\n",
    "\n",
    "remove some and REPHRASE EVERYTHING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a447b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Load the pre-trained FastText model (English) from Gensim's API\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3f13b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 2678\n",
      "Some OOV words: ['<UNK>', 'jeanclaud', 'damme', 'cowriterdirector', 'tolkiens', 'middleearth', 'tootepid', 'wisegirls', 'familyoriented', 'fantasyadventure']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load your models and define vocab\n",
    "\n",
    "vocab = ['<UNK>'] + vocab  # Adding a padding token and a unk token\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))  # Initialize embedding matrix\n",
    "\n",
    "# Create a clean_word-to-index dictionary for your vocabulary\n",
    "word_to_idx = { word : idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "oov_random_embeds = []\n",
    "\n",
    "# Fill the embedding matrix\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[idx] = word2vec_model[word]  # Use Word2Vec vector\n",
    "    elif word in fasttext_model:\n",
    "        embedding_matrix[idx] = fasttext_model[word]  # Use FastText vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random vector for OOV words\n",
    "        oov_random_embeds.append(word)\n",
    "\n",
    "        \n",
    "# Check for OOV words\n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_random_embeds)))\n",
    "print(\"Some OOV words:\", oov_random_embeds[:10])  # Print the first 10 OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d2253-9ece-4fc3-87cb-68892e5e605e",
   "metadata": {},
   "source": [
    "## Part 2 Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200fac77-0b01-4fdc-9803-d8add50021ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word2vec vectors : (300,)\n"
     ]
    }
   ],
   "source": [
    "#print(\"Shape of word2vec vectors : \" + str(word2vec_model['great'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164fce6b-4953-4f3d-821a-67b3e4dfa74c",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "095a88ff-20f4-418d-a40f-3c155ceba1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence):\n",
    "    encoded_list = []\n",
    "    tokens = preprocess_text(sentence)\n",
    "    for token in tokens: \n",
    "        if token in word_to_idx:\n",
    "            encoded_list.append(word_to_idx[token])\n",
    "        else:\n",
    "            encoded_list.append(1)# index of <UNK>\n",
    "\n",
    "    # padding : .append(0)\n",
    "    return encoded_list\n",
    "    \n",
    "# DataLoader\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_dataset if split == 'train' else validation_dataset\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "    \n",
    "    x_numerical = []\n",
    "    for i in ix:\n",
    "        sentence = data[i.item()]['text']\n",
    "        indices = encode(sentence)  \n",
    "        x_numerical.append(torch.tensor(indices))  \n",
    "    \n",
    "    y = torch.tensor([data[i.item()]['label'] for i in ix], dtype=torch.long) \n",
    "    x_numerical = [x.to(device) for x in x_numerical] \n",
    "    y = y.to(device)\n",
    "    \n",
    "    return x_numerical, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        accuracies = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            sequences, labels = get_batch(split)\n",
    "            padded_sequences = pad_sequence(sequences, batch_first=True)  \n",
    "            lengths = torch.tensor([seq.size(0) for seq in sequences])\n",
    "            \n",
    "            # Get logits and loss from the model\n",
    "            logits, loss, _ = model(padded_sequences, lengths, labels)\n",
    "            losses[k] = loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(logits, dim=1)  # Get predicted class indices\n",
    "            correct = (predictions == labels).float()  # Compare predictions with labels\n",
    "            accuracies[k] = correct.sum() / labels.size(0)  # Calculate accuracy for the batch\n",
    "\n",
    "        out[split] = {\n",
    "            'loss': losses.mean(),\n",
    "            'accuracy': accuracies.mean()  # Mean accuracy across iterations\n",
    "        }\n",
    "    \n",
    "    model.train()  # Switch back to training mode\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d93cf6-f022-4d40-bf52-2ffbb804b2db",
   "metadata": {},
   "source": [
    "### Using Custom RNN : SLOW AF AND LOSS DOES NOT CONVERGE!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1041fba0-5829-4d6c-b1c7-5b7ead06e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Weight matrices\n",
    "        self.Wx = nn.Parameter(torch.Tensor(input_size, hidden_size))  # Input to hidden\n",
    "        self.Wh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))  # Hidden to hidden\n",
    "        self.b = nn.Parameter(torch.Tensor(hidden_size))  # Bias\n",
    "\n",
    "        # LayerNorm for hidden state\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.kaiming_uniform_(self.Wx)\n",
    "            nn.init.kaiming_uniform_(self.Wh)\n",
    "            nn.init.zeros_(self.b)\n",
    "        # LayerNorm has two parameters: weight (gamma) and bias (beta)\n",
    "        self.layer_norm.weight.data.fill_(1)  # Initialize layer norm weight to 1\n",
    "        self.layer_norm.bias.data.fill_(0)    # Initialize layer norm bias to 0\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # x: input at the current time step (batch_size, input_size)\n",
    "        # h: hidden state from the previous time step (batch_size, hidden_size)\n",
    "        h_next = torch.tanh(torch.mm(x.view(1, -1), self.Wx) + torch.mm(h.view(1, -1), self.Wh) + self.b)\n",
    "        h_next = self.layer_norm(h_next)\n",
    "        return h_next.view(-1)  # Return to shape (hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c8f5d3c-5b5c-46c6-ab95-6905801c0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = SimpleRNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        pre_trained_embeddings = torch.tensor(embedding_matrix)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_size)\n",
    "        self.token_embedding_table.weight.data.copy_(pre_trained_embeddings)\n",
    "        self.token_embedding_table.weight.requires_grad = False # Cuz we dont want to update word embeds\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, lengths, labels=None):\n",
    "        batch_size = x.size(0)\n",
    "        max_length = x.size(1)\n",
    "\n",
    "        x = self.token_embedding_table(x)\n",
    "\n",
    "        h = torch.zeros(batch_size, self.hidden_size).to(x.device) # Starting Fresh for Every Batch of Sequences\n",
    "\n",
    "        for t in range(max_length):\n",
    "            for i in range(batch_size):\n",
    "                if t < lengths[i]:\n",
    "                    h_next = self.rnn_cell(x[i, t, :], h[i])\n",
    "                    h[i] = h_next.detach() # Detach to prevent in-place modifications\n",
    "                    \n",
    "        final_h = h\n",
    "        final_h = self.layer_norm(final_h)\n",
    "        logits = self.fc(final_h)\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            predictions = torch.argmax(logits, dim=1) # Get predicted class by taking argmax over logits\n",
    "            correct = (predictions == labels).sum().item()  \n",
    "            accuracy = correct / batch_size  \n",
    "            \n",
    "        return logits, loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f2e37-138c-4579-afb8-9b1d1b590ac1",
   "metadata": {},
   "source": [
    "### Using Torch's RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f8ededcf-b61a-4358-a1f1-2a958fa8e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TorchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TorchRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, feature_size)\n",
    "        pre_trained_embeddings = torch.tensor(embedding_matrix)\n",
    "        self.token_embedding_table.weight.data.copy_(pre_trained_embeddings)\n",
    "        self.token_embedding_table.weight.requires_grad = False  # Freeze embeddings\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths, labels=None):\n",
    "        batch_size = x.size(0)\n",
    "        max_length = x.size(1)\n",
    "\n",
    "        # Retrieve embeddings\n",
    "        x = self.token_embedding_table(x)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)  # Initial hidden state\n",
    "\n",
    "        # Pack the padded sequence\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Forward pass through RNN\n",
    "        packed_output, h_n = self.rnn(packed_x, h_0)\n",
    "\n",
    "        # Unpack the output and get the last hidden state\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        final_h = h_n[-1]  # Last hidden state of the last layer\n",
    "\n",
    "        # Layer normalization\n",
    "        final_h = self.layer_norm(final_h)\n",
    "\n",
    "        # Fully connected layer\n",
    "        logits = self.fc(final_h)\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "            accuracy = None\n",
    "        else:\n",
    "            # Calculate loss and accuracy\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            predictions = torch.argmax(logits, dim=1)  # Get predicted class by taking argmax over logits\n",
    "            correct = (predictions == labels).sum().item()  \n",
    "            accuracy = correct / batch_size  \n",
    "\n",
    "        return logits, loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a92ea-6e72-436a-9f7f-c2b6c7346005",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "146e82c6-f2fd-46df-a316-f0c8ab7e71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "6.009002 M parameters\n",
      "step 0: train loss 0.7550, train accuracy 0.4875, val loss 0.7692, val accuracy 0.4769\n",
      "step 500: train loss 0.5745, train accuracy 0.7050, val loss 0.6433, val accuracy 0.6269\n",
      "step 1000: train loss 0.5135, train accuracy 0.7538, val loss 0.5589, val accuracy 0.7212\n",
      "step 1500: train loss 0.4660, train accuracy 0.7862, val loss 0.5636, val accuracy 0.7244\n",
      "step 2000: train loss 0.4639, train accuracy 0.7937, val loss 0.5553, val accuracy 0.7188\n",
      "step 2500: train loss 0.4629, train accuracy 0.7969, val loss 0.5368, val accuracy 0.7475\n",
      "step 3000: train loss 0.4433, train accuracy 0.7975, val loss 0.5751, val accuracy 0.7113\n",
      "step 3500: train loss 0.4258, train accuracy 0.7987, val loss 0.5695, val accuracy 0.7081\n",
      "step 4000: train loss 0.4067, train accuracy 0.8131, val loss 0.5831, val accuracy 0.7125\n",
      "step 4500: train loss 0.4218, train accuracy 0.8125, val loss 0.5686, val accuracy 0.7312\n",
      "step 5000: train loss 0.4156, train accuracy 0.8175, val loss 0.5777, val accuracy 0.7175\n",
      "step 5500: train loss 0.3824, train accuracy 0.8363, val loss 0.5795, val accuracy 0.7287\n",
      "step 6000: train loss 0.3998, train accuracy 0.8300, val loss 0.6356, val accuracy 0.6894\n",
      "step 6500: train loss 0.3836, train accuracy 0.8269, val loss 0.6173, val accuracy 0.7050\n",
      "step 6663: train loss 0.3606, train accuracy 0.8450, val loss 0.6349, val accuracy 0.7088\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Data characteristics\n",
    "feature_size = 300\n",
    "num_classes = 2\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = feature_size\n",
    "hidden_size = feature_size * 2  # Size of the hidden layer\n",
    "output_size = num_classes  # Number of output classes\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "epochs = 25\n",
    "max_iters = round(len(train_dataset)/batch_size * epochs)\n",
    "eval_iters = 50 # Number of mini-batches we want to evaluate\n",
    "eval_interval = 500 # How many iterations before we evaluate\n",
    "\n",
    "model = TorchRNN(input_size, hidden_size, output_size)\n",
    "model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']['loss']:.4f}, train accuracy {losses['train']['accuracy']:.4f}, \"\n",
    "              f\"val loss {losses['val']['loss']:.4f}, val accuracy {losses['val']['accuracy']:.4f}\")\n",
    "    # sample a batch of data\n",
    "    sequences, labels = get_batch('train')\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)  # shape: (batch_size, max_length, features)\n",
    "    lengths = torch.tensor([seq.size(0) for seq in sequences])  # lengths of each original sequence\n",
    "\n",
    "    # evaluate the loss\n",
    "    with torch.autocast(device_type=device,dtype=torch.bfloat16):\n",
    "        logits, loss, accuracy = model(padded_sequences, lengths, labels)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e47acabc-a0c3-4e1f-b315-be23e71103af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible improvements:\n",
    "# hyperparams lr etc..\n",
    "# add layer norm\n",
    "# use gelu\n",
    "# residual layer?? \n",
    "# drop thingy\n",
    "# how the words are aggregated to sentence as a whole.. but would require using custom from scratch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e99d9-a998-4978-818e-e6f59ef1f003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
