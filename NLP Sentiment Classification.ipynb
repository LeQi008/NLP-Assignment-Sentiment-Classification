{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ccf792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd0007",
   "metadata": {},
   "source": [
    "## Part 0 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6875bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021a239",
   "metadata": {},
   "source": [
    "## Part 1 Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf30b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# List all available pre-trained models\n",
    "available_models = api.info()['models'].keys()\n",
    "print(available_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c6c55",
   "metadata": {},
   "source": [
    "### Loading 'word2vec-google-news-300' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6ca8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model (300-dimensional vectors)\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8c3a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8530\n",
      "Element 0: text\n",
      "Element 1: label\n",
      "'text' : the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "'label' : 1\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "# View the first sample in the dataset\n",
    "first_sample = train_dataset[0]\n",
    "for i, key in enumerate(first_sample):\n",
    "    print(f\"Element {i}: {key}\")\n",
    "\n",
    "for key, value in first_sample.items():\n",
    "    print(f\"'{key}' : {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc9f215",
   "metadata": {},
   "source": [
    "### (a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f566d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18951\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    vocab_counter.update(sentence.split())  # Split sentences into words\n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bfd46",
   "metadata": {},
   "source": [
    "### (b) How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "538a2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 4585\n",
      "Sample of 10 OOV words: ['to', '21st', \"century's\", '\"', 'and', 'a', ',', 'jean-claud', 'damme', 'segal']\n"
     ]
    }
   ],
   "source": [
    "oov_words = []\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word2vec_model:\n",
    "        oov_words.append(word)  # Use pre-trained Word2Vec vector\n",
    "        \n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_words)))\n",
    "print(\"Sample of 10 OOV words: \" + str(oov_words[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8a0d3",
   "metadata": {},
   "source": [
    "### (c) Strategy to mitigate limitation of OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350ca9b",
   "metadata": {},
   "source": [
    "Each OOV word can be represented as a bag of character N-grams. Embeddings are then generated based on these N-grams. By representing words as combinations of n-grams, FastText can generalize better across similar words. For example, if the model has seen \"apple,\" it can infer meaningful representations for related words like \"apples,\" \"applet,\" and even misspellings or variations.FastTextâ€™s n-gram approach can capture semantic similarities between words that share similar character patterns. For instance, \"cat\" and \"cats\" will share common n-grams, leading to embeddings that are close to each other in the vector space. For instance:\n",
    "The word \"unhappiness\" can be broken down into n-grams like \"un,\" \"happi,\" \"ness,\" etc.\n",
    "This enables FastText to understand that \"unhappy\" and \"happiness\" share a common root, even if those specific words were not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a346df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Load the pre-trained FastText model (English) from Gensim's API\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad77667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "embedding_dim = 300  # Since Word2Vec uses 300-dimensional vectors\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))  # Embedding matrix initialization\n",
    "\n",
    "# Create a word-to-index dictionary for your vocabulary\n",
    "word_to_idx = {word : idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Fill the embedding matrix\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[idx] = word2vec_model[word]  # Use pre-trained Word2Vec vector\n",
    "    else:\n",
    "        #embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random vector for OOV words\n",
    "        embedding_matrix[idx] = fasttext_model[word]  # Use FastText vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c613ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
