{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b66d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225433e",
   "metadata": {},
   "source": [
    "## Part 0 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cdc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45efb85a",
   "metadata": {},
   "source": [
    "## Part 1 Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a6291a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# List all available pre-trained models\n",
    "available_models = api.info()['models'].keys()\n",
    "print(available_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b196a4",
   "metadata": {},
   "source": [
    "### Loading 'word2vec-google-news-300' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a5f4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model (300-dimensional vectors)\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f94e493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8530\n",
      "Element 0: text\n",
      "Element 1: label\n",
      "'text' : the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "'label' : 1\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dataset\n",
    "print(len(train_dataset))\n",
    "\n",
    "# View the first sample in the dataset\n",
    "first_sample = train_dataset[0]\n",
    "for i, key in enumerate(first_sample):\n",
    "    print(f\"Element {i}: {key}\")\n",
    "\n",
    "for key, value in first_sample.items():\n",
    "    print(f\"'{key}' : {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e981acb",
   "metadata": {},
   "source": [
    "### (a) What is the size of the vocabulary formed from your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7feed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18951\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    vocab_counter.update(sentence.split())  # Split sentences into words\n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67723ef9-09dd-4672-839e-d79184356444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data vocabulary: 18223\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "vocab_counter = Counter()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = text.split()  \n",
    "    return tokens\n",
    "\n",
    "for sentence in train_dataset['text']:  \n",
    "    tokens = preprocess_text(sentence)\n",
    "    vocab_counter.update(tokens)  \n",
    "\n",
    "# Extract vocabulary\n",
    "vocab = list(vocab_counter.keys())\n",
    "#print(vocab)\n",
    "print(\"Size of training data vocabulary: \" + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38974c",
   "metadata": {},
   "source": [
    "### (b) How many OOV words exist in your training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c65b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 3334\n",
      "Sample of 10 OOV words: ['to', '21st', 'centurys', 'and', 'a', 'jeanclaud', 'damme', 'segal', 'of', 'cowriterdirector']\n"
     ]
    }
   ],
   "source": [
    "oov_words = []\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word2vec_model:\n",
    "        oov_words.append(word)  # Use pre-trained Word2Vec vector\n",
    "        \n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_words)))\n",
    "print(\"Sample of 10 OOV words: \" + str(oov_words[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645726e4",
   "metadata": {},
   "source": [
    "### (c) Strategy to mitigate limitation of OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefdf64",
   "metadata": {},
   "source": [
    "Each OOV word can be represented as a bag of character N-grams. Embeddings are then generated based on these N-grams. By representing words as combinations of n-grams, FastText can generalize better across similar words. For example, if the model has seen \"apple,\" it can infer meaningful representations for related words like \"apples,\" \"applet,\" and even misspellings or variations.FastTextâ€™s n-gram approach can capture semantic similarities between words that share similar character patterns. For instance, \"cat\" and \"cats\" will share common n-grams, leading to embeddings that are close to each other in the vector space. For instance:\n",
    "The word \"unhappiness\" can be broken down into n-grams like \"un,\" \"happi,\" \"ness,\" etc.\n",
    "This enables FastText to understand that \"unhappy\" and \"happiness\" share a common root, even if those specific words were not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a447b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Load the pre-trained FastText model (English) from Gensim's API\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3f13b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words in training data: 2679\n",
      "Some OOV words: ['<PAD>', '<UNK>', 'jeanclaud', 'damme', 'cowriterdirector', 'tolkiens', 'middleearth', 'tootepid', 'wisegirls', 'familyoriented']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load your models and define vocab\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>'] + vocab  # Adding a padding token and a unk token\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))  # Initialize embedding matrix\n",
    "\n",
    "# Create a clean_word-to-index dictionary for your vocabulary\n",
    "word_to_idx = { word : idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "oov_random_embeds = []\n",
    "\n",
    "# Fill the embedding matrix\n",
    "for word, idx in word_to_idx.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[idx] = word2vec_model[word]  # Use Word2Vec vector\n",
    "    elif word in fasttext_model:\n",
    "        embedding_matrix[idx] = fasttext_model[word]  # Use FastText vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random vector for OOV words\n",
    "        oov_random_embeds.append(word)\n",
    "\n",
    "        \n",
    "# Check for OOV words\n",
    "print(\"Number of OOV words in training data: \" + str(len(oov_random_embeds)))\n",
    "print(\"Some OOV words:\", oov_random_embeds[:10])  # Print the first 10 OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d2253-9ece-4fc3-87cb-68892e5e605e",
   "metadata": {},
   "source": [
    "## Part 2 Model Training & Evaluation - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200fac77-0b01-4fdc-9803-d8add50021ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word2vec vectors : (300,)\n"
     ]
    }
   ],
   "source": [
    "#print(\"Shape of word2vec vectors : \" + str(word2vec_model['great'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "095a88ff-20f4-418d-a40f-3c155ceba1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 59\n",
      "Min sequence length: 1\n",
      "Average sequence length: 20.99284876905041\n",
      "[12, 13, 14, 15, 16, 17, 12, 18, 19, 20, 21, 22, 23, 24, 25, 16, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    }
   ],
   "source": [
    "def find_seq_length(sentence):\n",
    "    sentencelist = sentence.split()\n",
    "    return len(sentencelist)\n",
    "\n",
    "def encode(sentence):\n",
    "    encoded_list = []\n",
    "    tokens = preprocess_text(sentence)\n",
    "    for token in tokens: # truncate? [:max_sequence_length]\n",
    "        if token in word_to_idx:\n",
    "            encoded_list.append(word_to_idx[token])\n",
    "        else:\n",
    "            encoded_list.append(1)# index of <UNK>\n",
    "\n",
    "    # padding : .append(0)\n",
    "    return encoded_list\n",
    "\n",
    "maxlength = 0\n",
    "minlength = float('inf')\n",
    "total_length = 0\n",
    "for i in range(len(train_dataset)):\n",
    "    cur = find_seq_length(train_dataset[i]['text'])\n",
    "    total_length += cur\n",
    "    if cur > maxlength:\n",
    "        maxlength = cur\n",
    "    if cur < minlength:\n",
    "        minlength = cur\n",
    "\n",
    "print(\"Max sequence length: \" + str(maxlength))\n",
    "print(\"Min sequence length: \" + str(minlength))\n",
    "print(\"Average sequence length: \" + str(total_length/len(train_dataset)))\n",
    "\n",
    "print(encode(train_dataset[0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59fe6d20-699d-465e-b0f0-447aff256505",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2262991417.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    time_size = max length\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Shape of word2vec vectors : \" + str(word2vec_model['great'].shape))\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32     \n",
    "time_size = max length ?\n",
    "feature_size =  word2vec_model['great'].shape[0]\n",
    "#hidden_size = 4 * 300   \n",
    "output_size = 1    # Output size (1 value)\n",
    "learning_rate = 0.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041fba0-5829-4d6c-b1c7-5b7ead06e603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
